{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /usr/local/anaconda3/lib/python3.9/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bb89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb57db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c70e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tania/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tania/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-49f8046d-3e2d-4256-b247-974f3209c50d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.2 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 412ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.2 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-49f8046d-3e2d-4256-b247-974f3209c50d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/34ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/08 19:47:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.13 (main, Aug 25 2022 18:29:29)\n",
      "Spark context Web UI available at http://192.168.0.25:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1691534859990).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#executing the shell.py inside the 'SPARK_HOME' environment\n",
    "\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"],'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f229fb1",
   "metadata": {},
   "source": [
    "# Creating Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcab3d3",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|Prashant|\n",
      "|  30|   Abdul|\n",
      "|  19|  Justin|\n",
      "|  43|    Andy|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.read.json('/Users/tania/Downloads/spark/Spark-Programming-In-Python-master/notebook/data/people.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b378733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date_df(df, fmt, field):\n",
    "    return df.withColumn(field, to_date(col(field), fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef672d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|  1|  4/5/2020|\n",
      "|  2|08/15/2022|\n",
      "|  3|01/01/2023|\n",
      "|  4|12/31/2022|\n",
      "+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_schema = StructType([\n",
    "    StructField('ID', StringType()),\n",
    "    StructField('EventDate', StringType())\n",
    "])\n",
    "\n",
    "my_rows = [\n",
    "    Row('1', '4/5/2020'),\n",
    "    Row('2', '08/15/2022'),\n",
    "    Row('3', '01/01/2023'),\n",
    "    Row('4', '12/31/2022')\n",
    "]\n",
    "\n",
    "my_rdd = spark.sparkContext.parallelize(my_rows)\n",
    "my_df = spark.createDataFrame(my_rdd, my_schema)\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea446050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- EventDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = to_date_df(my_df, 'M/d/y', 'EventDate')\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6edb6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|  1|2020-04-05|\n",
      "|  2|2022-08-15|\n",
      "|  3|2023-01-01|\n",
      "|  4|2022-12-31|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5e97",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f72afcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|  84|\n",
      "|  Pedro| 31|    1|  84|\n",
      "|  Maria| 15|    5|   1|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    ('Tania', 3,11,1987),\n",
    "    ('Daniel', 12,12,1980),\n",
    "    ('Juliana', 31,1,1983),\n",
    "    ('Pedro', 31,1,84),\n",
    "    ('Pedro', 31,1,84),\n",
    "    ('Maria', 15,5,1) # duplicate\n",
    "]\n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF('Name', 'Day', 'Month', 'Year')\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6da6112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning data\n",
    "raw_df_cleaned = raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then cast(Year as int) + 2000\n",
    "     when Year <100 then cast(Year as int) + 1900\n",
    "     else Year \n",
    "end'''))\n",
    "raw_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "242d4831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Day: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b539fd",
   "metadata": {},
   "source": [
    "# Creating new columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cb654",
   "metadata": {},
   "source": [
    "## with expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85e7736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "new_df2 = new_df.select('EventDate', \\\n",
    "                       year('EventDate').alias('Year'), \\\n",
    "                       month('EventDate').alias('Month'), \\\n",
    "                       dayofmonth(\"EventDate\").alias('day') )\n",
    "new_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95180875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----------+\n",
      "| EventDate|Year|Month|day|  Birthday|\n",
      "+----------+----+-----+---+----------+\n",
      "|2020-04-05|2020|    4|  5|2020-04-05|\n",
      "|2022-08-15|2022|    8| 15|2022-08-15|\n",
      "|2023-01-01|2023|    1|  1|2023-01-01|\n",
      "|2022-12-31|2022|   12| 31|2022-12-31|\n",
      "+----------+----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.withColumn('Birthday', expr('''to_date(concat(Year, '-', Month, '-', day))''')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f965d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----------+\n",
      "| EventDate|Year|Month|day|  Birthday|\n",
      "+----------+----+-----+---+----------+\n",
      "|2020-04-05|2020|    4|  5|2020-04-05|\n",
      "|2022-08-15|2022|    8| 15|2022-08-15|\n",
      "|2023-01-01|2023|    1|  1|2023-01-01|\n",
      "|2022-12-31|2022|   12| 31|2022-12-31|\n",
      "+----------+----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.withColumn('Birthday', to_date(expr('''concat(Year, '-', Month, '-', day)'''))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8846d",
   "metadata": {},
   "source": [
    "## drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e37ec3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "| EventDate|Year|\n",
      "+----------+----+\n",
      "|2020-04-05|2020|\n",
      "|2022-08-15|2022|\n",
      "|2023-01-01|2023|\n",
      "|2022-12-31|2022|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.drop('day', 'Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e17bce",
   "metadata": {},
   "source": [
    "## drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "808ff7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.drop_duplicates(['Year']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528d601",
   "metadata": {},
   "source": [
    "## create a unique id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f741f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+----------+\n",
      "|   Name|Day|Month|Year|        id|\n",
      "+-------+---+-----+----+----------+\n",
      "|Juliana| 31|    1|1983|         0|\n",
      "|  Pedro| 31|    1|  84|         1|\n",
      "|  Tania|  3|   11|1987|8589934592|\n",
      "| Daniel| 12|   12|1980|8589934593|\n",
      "+-------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "raw_df_partitioned = raw_df.repartition(2)\n",
    "raw_df_partitioned_with_id = raw_df_partitioned.withColumn('id', monotonically_increasing_id())\n",
    "raw_df_partitioned_with_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d2c23",
   "metadata": {},
   "source": [
    "## sort column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9e0c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2023-01-01|2023|    1|  1|\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.sort(expr('Month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e552a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7d7f4",
   "metadata": {},
   "source": [
    "## Substring Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33ada1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='b.c.d')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
    "df.select(substring_index(df.s, '.', 1).alias('s')).collect()\n",
    "df.select(substring_index(df.s, '.', -3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712adc22",
   "metadata": {},
   "source": [
    "## Inline Column Type Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89edc6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with cast() as expression\n",
    "raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then cast(Year as int) + 2000\n",
    "     when Year <100 then cast(Year as int) + 1900\n",
    "     else Year \n",
    "end''')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f1bc390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.withColumn('Year', \\\n",
    "                  when(col('Year') < 23, col('Year') + 2000) \\\n",
    "                  .when(col('Year') < 100, col('Year') + 1900)\\\n",
    "                  .otherwise(col('Year'))\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f818f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with cast() function\n",
    "raw_df_cleaned = raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then Year + 2000\n",
    "     when Year <100 then Year + 1900\n",
    "     else Year \n",
    "end''').cast(IntegerType()))\n",
    "raw_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
