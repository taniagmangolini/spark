{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /usr/local/anaconda3/lib/python3.9/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bb89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb57db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c70e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tania/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tania/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cbc73c42-f349-4348-9ffe-5332d9935cf1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.2 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 364ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.2 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cbc73c42-f349-4348-9ffe-5332d9935cf1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/09 20:15:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.13 (main, Aug 25 2022 18:29:29)\n",
      "Spark context Web UI available at http://192.168.0.25:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1691622919291).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#executing the shell.py inside the 'SPARK_HOME' environment\n",
    "\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"],'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f229fb1",
   "metadata": {},
   "source": [
    "# Creating Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcab3d3",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|Prashant|\n",
      "|  30|   Abdul|\n",
      "|  19|  Justin|\n",
      "|  43|    Andy|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.read.json('/Users/tania/Downloads/spark/Spark-Programming-In-Python-master/notebook/data/people.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b378733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date_df(df, fmt, field):\n",
    "    return df.withColumn(field, to_date(col(field), fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef672d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|  1|  4/5/2020|\n",
      "|  2|08/15/2022|\n",
      "|  3|01/01/2023|\n",
      "|  4|12/31/2022|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_schema = StructType([\n",
    "    StructField('ID', StringType()),\n",
    "    StructField('EventDate', StringType())\n",
    "])\n",
    "\n",
    "my_rows = [\n",
    "    Row('1', '4/5/2020'),\n",
    "    Row('2', '08/15/2022'),\n",
    "    Row('3', '01/01/2023'),\n",
    "    Row('4', '12/31/2022')\n",
    "]\n",
    "\n",
    "my_rdd = spark.sparkContext.parallelize(my_rows)\n",
    "my_df = spark.createDataFrame(my_rdd, my_schema)\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea446050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- EventDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = to_date_df(my_df, 'M/d/y', 'EventDate')\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edb6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|  1|2020-04-05|\n",
      "|  2|2022-08-15|\n",
      "|  3|2023-01-01|\n",
      "|  4|2022-12-31|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5e97",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72afcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|  84|\n",
      "|  Pedro| 31|    1|  84|\n",
      "|  Maria| 15|    5|   1|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    ('Tania', 3,11,1987),\n",
    "    ('Daniel', 12,12,1980),\n",
    "    ('Juliana', 31,1,1983),\n",
    "    ('Pedro', 31,1,84),\n",
    "    ('Pedro', 31,1,84),\n",
    "    ('Maria', 15,5,1) # duplicate\n",
    "]\n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF('Name', 'Day', 'Month', 'Year')\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da6112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning data\n",
    "raw_df_cleaned = raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then cast(Year as int) + 2000\n",
    "     when Year <100 then cast(Year as int) + 1900\n",
    "     else Year \n",
    "end'''))\n",
    "raw_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242d4831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Day: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b539fd",
   "metadata": {},
   "source": [
    "# Creating new columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cb654",
   "metadata": {},
   "source": [
    "## with expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e7736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "new_df2 = new_df.select('EventDate', \\\n",
    "                       year('EventDate').alias('Year'), \\\n",
    "                       month('EventDate').alias('Month'), \\\n",
    "                       dayofmonth(\"EventDate\").alias('day') )\n",
    "new_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95180875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----------+\n",
      "| EventDate|Year|Month|day|  Birthday|\n",
      "+----------+----+-----+---+----------+\n",
      "|2020-04-05|2020|    4|  5|2020-04-05|\n",
      "|2022-08-15|2022|    8| 15|2022-08-15|\n",
      "|2023-01-01|2023|    1|  1|2023-01-01|\n",
      "|2022-12-31|2022|   12| 31|2022-12-31|\n",
      "+----------+----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.withColumn('Birthday', expr('''to_date(concat(Year, '-', Month, '-', day))''')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f965d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----------+\n",
      "| EventDate|Year|Month|day|  Birthday|\n",
      "+----------+----+-----+---+----------+\n",
      "|2020-04-05|2020|    4|  5|2020-04-05|\n",
      "|2022-08-15|2022|    8| 15|2022-08-15|\n",
      "|2023-01-01|2023|    1|  1|2023-01-01|\n",
      "|2022-12-31|2022|   12| 31|2022-12-31|\n",
      "+----------+----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.withColumn('Birthday', to_date(expr('''concat(Year, '-', Month, '-', day)'''))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8846d",
   "metadata": {},
   "source": [
    "## drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37ec3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "| EventDate|Year|\n",
      "+----------+----+\n",
      "|2020-04-05|2020|\n",
      "|2022-08-15|2022|\n",
      "|2023-01-01|2023|\n",
      "|2022-12-31|2022|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.drop('day', 'Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e17bce",
   "metadata": {},
   "source": [
    "## drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808ff7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.drop_duplicates(['Year']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528d601",
   "metadata": {},
   "source": [
    "## create a unique id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f741f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+----------+\n",
      "|   Name|Day|Month|Year|        id|\n",
      "+-------+---+-----+----+----------+\n",
      "|Juliana| 31|    1|1983|         0|\n",
      "|  Pedro| 31|    1|  84|         1|\n",
      "|  Maria| 15|    5|   1|         2|\n",
      "|  Tania|  3|   11|1987|8589934592|\n",
      "| Daniel| 12|   12|1980|8589934593|\n",
      "|  Pedro| 31|    1|  84|8589934594|\n",
      "+-------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "raw_df_partitioned = raw_df.repartition(2)\n",
    "raw_df_partitioned_with_id = raw_df_partitioned.withColumn('id', monotonically_increasing_id())\n",
    "raw_df_partitioned_with_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d2c23",
   "metadata": {},
   "source": [
    "## sort column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9e0c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2023-01-01|2023|    1|  1|\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.sort(expr('Month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e552a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "| EventDate|Year|Month|day|\n",
      "+----------+----+-----+---+\n",
      "|2020-04-05|2020|    4|  5|\n",
      "|2022-08-15|2022|    8| 15|\n",
      "|2022-12-31|2022|   12| 31|\n",
      "|2023-01-01|2023|    1|  1|\n",
      "+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7d7f4",
   "metadata": {},
   "source": [
    "## Substring Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33ada1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='b.c.d')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
    "df.select(substring_index(df.s, '.', 1).alias('s')).collect()\n",
    "df.select(substring_index(df.s, '.', -3).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712adc22",
   "metadata": {},
   "source": [
    "## Inline Column Type Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89edc6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with cast() as expression\n",
    "raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then cast(Year as int) + 2000\n",
    "     when Year <100 then cast(Year as int) + 1900\n",
    "     else Year \n",
    "end''')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f1bc390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.withColumn('Year', \\\n",
    "                  when(col('Year') < 23, col('Year') + 2000) \\\n",
    "                  .when(col('Year') < 100, col('Year') + 1900)\\\n",
    "                  .otherwise(col('Year'))\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f818f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+\n",
      "|   Name|Day|Month|Year|\n",
      "+-------+---+-----+----+\n",
      "|  Tania|  3|   11|1987|\n",
      "| Daniel| 12|   12|1980|\n",
      "|Juliana| 31|    1|1983|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Pedro| 31|    1|1984|\n",
      "|  Maria| 15|    5|2001|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with cast() function\n",
    "raw_df_cleaned = raw_df.withColumn('Year', \n",
    "expr(\n",
    "'''case \n",
    "     when Year < 23 then Year + 2000\n",
    "     when Year <100 then Year + 1900\n",
    "     else Year \n",
    "end''').cast(IntegerType()))\n",
    "raw_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cb701",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0abb51fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|      _c0|      _c1|                 _c2|     _c3|            _c4|      _c5|       _c6|           _c7|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "invoices = spark.read.csv('/Users/tania/PycharmProjects/MyFirstSparkProgram/data/invoices.csv').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "040e0b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "invoices_df = spark.read\\\n",
    "                   .format('csv')\\\n",
    "                   .option('header', 'true')\\\n",
    "                   .option('inferSchema', 'true')\\\n",
    "                   .load('/Users/tania/PycharmProjects/MyFirstSparkProgram/data/invoices.csv')\n",
    "invoices_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b864bd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+-------------+\n",
      "|Count *|TotalCount|     AveragePrice|CountDistinct|\n",
      "+-------+----------+-----------------+-------------+\n",
      "| 541909|   5176450|4.611113626088501|        25900|\n",
      "+-------+----------+-----------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "invoices_df.select(f.count('*').alias('Count *'),\n",
    "                   f.sum('Quantity').alias('TotalCount'),\n",
    "                   f.avg('UnitPrice').alias('AveragePrice'),\n",
    "                   f.countDistinct('InvoiceNo').alias('CountDistinct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way\n",
    "invoices_df.selectExpr(\n",
    "    'count(1) as  count 1', # count 1 is similar to * and include null\n",
    "    'count(StockCode) as count field', # do not include nulls\n",
    "    'sum(Quantity) as TotalQuantity',\n",
    "    'avg(UnitPrice) as AveragePrice'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ec4a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+-----------------+\n",
      "|count 1|count field|TotalQuantity|     AveragePrice|\n",
      "+-------+-----------+-------------+-----------------+\n",
      "| 541909|     541908|      5176450|4.611113626088323|\n",
      "+-------+-----------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way\n",
    "invoices_df.selectExpr(\n",
    "    'count(1) as  `count 1`',\n",
    "    'count(StockCode) as `count field`', # do not include nulls\n",
    "    'sum(Quantity) as TotalQuantity',\n",
    "    'avg(UnitPrice) as AveragePrice'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "218d8d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+------------+\n",
      "|       Country|InvoiceNo|TotalQuantity|InvoiceValue|\n",
      "+--------------+---------+-------------+------------+\n",
      "|United Kingdom|   536446|          329|      440.89|\n",
      "|United Kingdom|   536508|          216|      155.52|\n",
      "|United Kingdom|   537018|           -3|         0.0|\n",
      "|United Kingdom|   537401|          -24|         0.0|\n",
      "|United Kingdom|   537811|           74|      268.86|\n",
      "|United Kingdom|  C537824|           -2|       -14.9|\n",
      "|United Kingdom|   538895|          370|      247.38|\n",
      "|United Kingdom|   540453|          341|      302.45|\n",
      "|United Kingdom|   541291|          217|      305.81|\n",
      "|United Kingdom|   542551|           -1|         0.0|\n",
      "|United Kingdom|   542576|           -1|         0.0|\n",
      "|United Kingdom|   542628|            9|      132.35|\n",
      "|United Kingdom|   542886|          199|      320.51|\n",
      "|United Kingdom|   542907|           75|      313.85|\n",
      "|United Kingdom|   543131|          134|       164.1|\n",
      "|United Kingdom|   543189|          102|      153.94|\n",
      "|United Kingdom|   543265|           -4|         0.0|\n",
      "|        Cyprus|   544574|          173|      320.69|\n",
      "|United Kingdom|   545077|           24|       10.08|\n",
      "|United Kingdom|   545300|          116|      323.16|\n",
      "+--------------+---------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# with SQL\n",
    "invoices_df.createOrReplaceTempView('sales')\n",
    "summary = spark.sql(\n",
    "''' SELECT Country, InvoiceNo,\n",
    "           sum(Quantity) as TotalQuantity,\n",
    "           round(sum(Quantity * UnitPrice), 2) as InvoiceValue\n",
    "    FROM sales\n",
    "    GROUP BY Country, InvoiceNo\n",
    "''')\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "187d843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+------------+\n",
      "|       Country|InvoiceNo|TotalQuantity|InvoiceValue|\n",
      "+--------------+---------+-------------+------------+\n",
      "|United Kingdom|   536446|          329|      440.89|\n",
      "|United Kingdom|   536508|          216|      155.52|\n",
      "|United Kingdom|   537018|           -3|         0.0|\n",
      "|United Kingdom|   537401|          -24|         0.0|\n",
      "|United Kingdom|   537811|           74|      268.86|\n",
      "|United Kingdom|  C537824|           -2|       -14.9|\n",
      "|United Kingdom|   538895|          370|      247.38|\n",
      "|United Kingdom|   540453|          341|      302.45|\n",
      "|United Kingdom|   541291|          217|      305.81|\n",
      "|United Kingdom|   542551|           -1|         0.0|\n",
      "|United Kingdom|   542576|           -1|         0.0|\n",
      "|United Kingdom|   542628|            9|      132.35|\n",
      "|United Kingdom|   542886|          199|      320.51|\n",
      "|United Kingdom|   542907|           75|      313.85|\n",
      "|United Kingdom|   543131|          134|       164.1|\n",
      "|United Kingdom|   543189|          102|      153.94|\n",
      "|United Kingdom|   543265|           -4|         0.0|\n",
      "|        Cyprus|   544574|          173|      320.69|\n",
      "|United Kingdom|   545077|           24|       10.08|\n",
      "|United Kingdom|   545300|          116|      323.16|\n",
      "+--------------+---------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# another way\n",
    "\n",
    "summary = invoices_df\\\n",
    "          .groupBy('Country', 'InvoiceNo')\\\n",
    "          .agg(f.sum('Quantity').alias('TotalQuantity'),\n",
    "               f.round(f.sum(f.expr('Quantity * UnitPrice')), 2).alias('InvoiceValue'))\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bdbd636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "|      Australia|        48|          1|          107|      358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|\n",
      "|      Australia|        50|          2|          133|      387.95|\n",
      "|        Austria|        50|          2|            3|      257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|\n",
      "|        Belgium|        51|          2|          942|      838.65|\n",
      "|Channel Islands|        49|          1|           80|      363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|\n",
      "|           EIRE|        51|          5|           95|      276.84|\n",
      "|        Finland|        50|          1|         1254|       892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|\n",
      "|         France|        50|          6|          529|      537.32|\n",
      "|         France|        51|          5|          847|     1702.87|\n",
      "+---------------+----------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating new columns\n",
    "\n",
    "num_of_invoices = f.countDistinct('InvoiceNo').alias('NumInvoices')\n",
    "total_quantity = f.sum('Quantity').alias('TotalQuantity')\n",
    "total_invoice = f.round(f.sum(f.expr('Quantity * UnitPrice')), 2).alias('InvoiceValue')\n",
    "\n",
    "summary_new = invoices_df\\\n",
    "              .withColumn('InvoiceDate', f.to_date(f.col('InvoiceDate'), 'dd-MM-yyyy H.mm'))\\\n",
    "              .where('year(InvoiceDate) == 2010')\\\n",
    "              .withColumn('WeekNumber', f.weekofyear(f.col('InvoiceDate')))\\\n",
    "              .groupBy('Country', 'WeekNumber')\\\n",
    "              .agg(num_of_invoices, total_quantity, total_invoice)\n",
    "\n",
    "# save result to parquet format\n",
    "# df.coalesce(1) will write to one file\n",
    "summary_new.coalesce(1).write\\\n",
    "           .format('parquet')\\\n",
    "           .mode('overwrite')\\\n",
    "           .save('output_dir')\n",
    "\n",
    "summary_new.sort('Country', 'WeekNumber').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fbfce",
   "metadata": {},
   "source": [
    "# Windowing Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92283175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|      RunningTotal|\n",
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "|      Australia|        48|          1|          107|      358.25|            358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|            617.15|\n",
      "|      Australia|        50|          2|          133|      387.95|1005.0999999999999|\n",
      "|        Austria|        50|          2|            3|      257.04|            257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|            205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|             346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|            971.26|\n",
      "|        Belgium|        51|          2|          942|      838.65|1809.9099999999999|\n",
      "|Channel Islands|        49|          1|           80|      363.53|            363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|           1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|            1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|           3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|           6431.33|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|           8753.11|\n",
      "|           EIRE|        51|          5|           95|      276.84|           9029.95|\n",
      "|        Finland|        50|          1|         1254|       892.8|             892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|           2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|           7335.17|\n",
      "|         France|        50|          6|          529|      537.32|           7872.49|\n",
      "|         France|        51|          5|          847|     1702.87|           9575.36|\n",
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "running_total_window = Window.partitionBy('Country')\\\n",
    "                             .orderBy('WeekNumber')\\\n",
    "                             .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# PS:\n",
    "# unboundedPreceding: from the beginning. Can be a numeric value\n",
    "\n",
    "summary_window = summary_new\\\n",
    "                 .withColumn('RunningTotal', \n",
    "                             sum('InvoiceValue').over(running_total_window))\n",
    "summary_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0ab90",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fdcaa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+-----------+\n",
      "|prod_id|          prod_name|list_price|reorder_qty|\n",
      "+-------+-------------------+----------+-----------+\n",
      "|     01|       Scroll Mouse|       250|         20|\n",
      "|     02|      Optical Mouse|       350|         20|\n",
      "|     03|     Wireless Mouse|       450|         50|\n",
      "|     04|  Wireless Keyboard|       580|         50|\n",
      "|     05|  Standard Keyboard|       360|         10|\n",
      "|     06|16 GB Flash Storage|       240|        100|\n",
      "|     07|32 GB Flash Storage|       320|         50|\n",
      "|     08|64 GB Flash Storage|       430|         25|\n",
      "+-------+-------------------+----------+-----------+\n",
      "\n",
      "+--------+-------+----------+---+\n",
      "|order_id|prod_id|unit_price|qty|\n",
      "+--------+-------+----------+---+\n",
      "|      01|     02|       350|  1|\n",
      "|      01|     04|       580|  1|\n",
      "|      01|     07|       320|  2|\n",
      "|      02|     03|       450|  1|\n",
      "|      02|     06|       220|  1|\n",
      "|      03|     01|       195|  1|\n",
      "|      04|     09|       270|  3|\n",
      "|      04|     08|       410|  2|\n",
      "|      05|     02|       350|  1|\n",
      "+--------+-------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_list = [(\"01\", \"02\", 350, 1),\n",
    "               (\"01\", \"04\", 580, 1),\n",
    "               (\"01\", \"07\", 320, 2),\n",
    "               (\"02\", \"03\", 450, 1),\n",
    "               (\"02\", \"06\", 220, 1),\n",
    "               (\"03\", \"01\", 195, 1),\n",
    "               (\"04\", \"09\", 270, 3),\n",
    "               (\"04\", \"08\", 410, 2),\n",
    "               (\"05\", \"02\", 350, 1)]\n",
    "\n",
    "order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
    "\n",
    "product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
    "                (\"02\", \"Optical Mouse\", 350, 20),\n",
    "                (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "                (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "                (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "                (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "                (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "                (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
    "\n",
    "product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
    "product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
    "\n",
    "product_renamed_df.show()\n",
    "order_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd23fd6",
   "metadata": {},
   "source": [
    "## Inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2734cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+----------+---+\n",
      "|order_id|prod_id|          prod_name|unit_price|list_price|qty|\n",
      "+--------+-------+-------------------+----------+----------+---+\n",
      "|      03|     01|       Scroll Mouse|       195|       250|  1|\n",
      "|      01|     02|      Optical Mouse|       350|       350|  1|\n",
      "|      05|     02|      Optical Mouse|       350|       350|  1|\n",
      "|      02|     03|     Wireless Mouse|       450|       450|  1|\n",
      "|      01|     04|  Wireless Keyboard|       580|       580|  1|\n",
      "|      02|     06|16 GB Flash Storage|       220|       240|  1|\n",
      "|      01|     07|32 GB Flash Storage|       320|       320|  2|\n",
      "|      04|     08|64 GB Flash Storage|       410|       430|  2|\n",
      "+--------+-------+-------------------+----------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_expr = order_df.prod_id == product_df.prod_id\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"inner\") \\\n",
    "        .drop(product_renamed_df.prod_id) \\\n",
    "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c849bd",
   "metadata": {},
   "source": [
    "## Outer join (Full Outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88042d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+----------+----+\n",
      "|order_id|prod_id|          prod_name|unit_price|list_price| qty|\n",
      "+--------+-------+-------------------+----------+----------+----+\n",
      "|    null|   null|  Standard Keyboard|      null|       360|null|\n",
      "|      01|     04|  Wireless Keyboard|       580|       580|   1|\n",
      "|      01|     02|      Optical Mouse|       350|       350|   1|\n",
      "|      01|     07|32 GB Flash Storage|       320|       320|   2|\n",
      "|      02|     03|     Wireless Mouse|       450|       450|   1|\n",
      "|      02|     06|16 GB Flash Storage|       220|       240|   1|\n",
      "|      03|     01|       Scroll Mouse|       195|       250|   1|\n",
      "|      04|     08|64 GB Flash Storage|       410|       430|   2|\n",
      "|      04|     09|               null|       270|      null|   3|\n",
      "|      05|     02|      Optical Mouse|       350|       350|   1|\n",
      "+--------+-------+-------------------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_expr = order_df.prod_id == product_df.prod_id\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"outer\") \\\n",
    "        .drop(product_renamed_df.prod_id) \\\n",
    "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
    "        .sort('order_id')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51701d8f",
   "metadata": {},
   "source": [
    "## Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5460da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+----------+---+\n",
      "|order_id|prod_id|          prod_name|unit_price|list_price|qty|\n",
      "+--------+-------+-------------------+----------+----------+---+\n",
      "|      01|     02|      Optical Mouse|       350|       350|  1|\n",
      "|      01|     07|32 GB Flash Storage|       320|       320|  2|\n",
      "|      01|     04|  Wireless Keyboard|       580|       580|  1|\n",
      "|      02|     03|     Wireless Mouse|       450|       450|  1|\n",
      "|      02|     06|16 GB Flash Storage|       220|       240|  1|\n",
      "|      03|     01|       Scroll Mouse|       195|       250|  1|\n",
      "|      04|     09|                 09|       270|       270|  3|\n",
      "|      04|     08|64 GB Flash Storage|       410|       430|  2|\n",
      "|      05|     02|      Optical Mouse|       350|       350|  1|\n",
      "+--------+-------+-------------------+----------+----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49514)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/tania/Downloads/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "join_expr = order_df.prod_id == product_df.prod_id\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"left\") \\\n",
    "        .drop(product_renamed_df.prod_id) \\\n",
    "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
    "        .withColumn('prod_name', expr('coalesce(prod_name, prod_id)'))\\\n",
    "        .withColumn('list_price', expr('coalesce(list_price, unit_price)'))\\\n",
    "        .sort('order_id')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e54e3",
   "metadata": {},
   "source": [
    "## Right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94c7a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+----------+----+\n",
      "|order_id|prod_id|          prod_name|unit_price|list_price| qty|\n",
      "+--------+-------+-------------------+----------+----------+----+\n",
      "|    null|   null|  Standard Keyboard|      null|       360|null|\n",
      "|      01|     07|32 GB Flash Storage|       320|       320|   2|\n",
      "|      01|     04|  Wireless Keyboard|       580|       580|   1|\n",
      "|      01|     02|      Optical Mouse|       350|       350|   1|\n",
      "|      02|     03|     Wireless Mouse|       450|       450|   1|\n",
      "|      02|     06|16 GB Flash Storage|       220|       240|   1|\n",
      "|      03|     01|       Scroll Mouse|       195|       250|   1|\n",
      "|      04|     08|64 GB Flash Storage|       410|       430|   2|\n",
      "|      05|     02|      Optical Mouse|       350|       350|   1|\n",
      "+--------+-------+-------------------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_expr = order_df.prod_id == product_df.prod_id\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"right\") \\\n",
    "        .drop(product_renamed_df.prod_id) \\\n",
    "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
    "        .sort('order_id')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0ff71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
